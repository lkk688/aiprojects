import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from datasets import load_dataset
import time
import math
from fla.layers import GatedLinearAttention  # æ ¸å¿ƒï¼šå¼•å…¥çº¿æ€§æ³¨æ„åŠ›å±‚

# ==========================================
# 1. é…ç½®ä¸è¶…å‚æ•°
# ==========================================
CONFIG = {
    "vocab_size": 50257,    # GPT-2 è¯è¡¨å¤§å°
    "d_model": 512,         # éšè—å±‚ç»´åº¦ (ä¿æŒä¸€è‡´ä»¥å…¬å¹³å¯¹æ¯”)
    "n_layers": 4,          # å±‚æ•°
    "n_heads": 8,           # å¤´æ•°
    "seq_len": 16384, #4096,        # åºåˆ—é•¿åº¦ (å…³é”®ï¼šè¶Šé•¿çº¿æ€§ä¼˜åŠ¿è¶Šæ˜æ˜¾)
    "batch_size": 1, #8,       # æ‰¹æ¬¡å¤§å°
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "steps": 20            # æ¯ä¸ªæ¨¡å‹è·‘å¤šå°‘æ­¥ (ç”¨äºæµ‹è¯•é€Ÿåº¦)
}

print(f"ğŸš€ Running on {CONFIG['device']} with SeqLen={CONFIG['seq_len']}")

# ==========================================
# 2. æ¨¡å‹å®šä¹‰
# ==========================================

# --- A. åŸºå‡† RNN (LSTM) ---
class RNNBaseline(nn.Module):
    def __init__(self):
        super().__init__()
        self.embed = nn.Embedding(CONFIG['vocab_size'], CONFIG['d_model'])
        # LSTM æ— æ³•å¹¶è¡Œè®­ç»ƒï¼Œé€šå¸¸å¾ˆæ…¢
        self.lstm = nn.LSTM(
            input_size=CONFIG['d_model'],
            hidden_size=CONFIG['d_model'],
            num_layers=CONFIG['n_layers'],
            batch_first=True
        )
        self.head = nn.Linear(CONFIG['d_model'], CONFIG['vocab_size'])

    def forward(self, x):
        x = self.embed(x)
        x, _ = self.lstm(x)
        return self.head(x)

# --- B. åŸºå‡† Transformer (O(N^2)) ---
class TransformerBaseline(nn.Module):
    def __init__(self):
        super().__init__()
        self.embed = nn.Embedding(CONFIG['vocab_size'], CONFIG['d_model'])
        self.pos_embed = nn.Parameter(torch.zeros(1, CONFIG['seq_len'], CONFIG['d_model']))
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=CONFIG['d_model'],
            nhead=CONFIG['n_heads'],
            dim_feedforward=CONFIG['d_model']*4,
            dropout=0.0,
            batch_first=True,
            norm_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=CONFIG['n_layers'])
        self.head = nn.Linear(CONFIG['d_model'], CONFIG['vocab_size'])

    def forward(self, x):
        B, T = x.size()
        # ç®€å•çš„ä½ç½®ç¼–ç æˆªæ–­
        pos = self.pos_embed[:, :T, :]
        x = self.embed(x) + pos
        # ç”Ÿæˆå› æœæ©ç  (Causal Mask)
        mask = nn.Transformer.generate_square_subsequent_mask(T).to(x.device)
        x = self.encoder(x, mask=mask, is_causal=True)
        return self.head(x)

# --- C. Hybrid/Linear Attention (O(N)) ---
# ä½¿ç”¨ Gated Linear Attention (GLA) - Qwen3/Mamba é£æ ¼çš„ä»£è¡¨
class HybridGLA(nn.Module):
    def __init__(self):
        super().__init__()
        self.embed = nn.Embedding(CONFIG['vocab_size'], CONFIG['d_model'])
        
        self.layers = nn.ModuleList([
            GLABlock(CONFIG['d_model'], CONFIG['n_heads']) 
            for _ in range(CONFIG['n_layers'])
        ])
        self.norm = nn.LayerNorm(CONFIG['d_model'])
        self.head = nn.Linear(CONFIG['d_model'], CONFIG['vocab_size'])

    def forward(self, x):
        x = self.embed(x)
        for layer in self.layers:
            x = layer(x)
        x = self.norm(x)
        return self.head(x)

class GLABlock(nn.Module):
    def __init__(self, hidden_size, num_heads):
        super().__init__()
        self.norm1 = nn.LayerNorm(hidden_size)
        # è¿™é‡Œè°ƒç”¨ fla åº“çš„ GatedLinearAttention
        # å®ƒæ˜¯ FlashAttention çš„çº¿æ€§å˜ä½“ï¼Œæ”¯æŒå¹¶è¡Œè®­ç»ƒå’Œé€’å½’æ¨ç†
        self.attn = GatedLinearAttention(
            hidden_size=hidden_size,
            num_heads=num_heads,
            mode='chunk' # H100 å¿…å¼€ï¼šä½¿ç”¨ Triton èåˆç®—å­åŠ é€Ÿ
        )
        self.norm2 = nn.LayerNorm(hidden_size)
        self.mlp = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.GELU(),
            nn.Linear(hidden_size * 4, hidden_size)
        )

    def forward(self, x):
        # è¿™é‡Œçš„ attention ä¸éœ€è¦ maskï¼Œå› ä¸º GLA å†…éƒ¨å¤„ç†äº†å› æœæ€§
        x = x + self.attn(self.norm1(x))[0]
        x = x + self.mlp(self.norm2(x))
        return x

# ==========================================
# 3. æ•°æ®åŠ è½½ (TinyStories)
# ==========================================
def get_dataloader():
    print("ğŸ“š Loading TinyStories dataset...")
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    
    # ä½¿ç”¨æµå¼åŠ è½½ï¼Œæ— éœ€ä¸‹è½½æ•´ä¸ªæ•°æ®é›†
    dataset = load_dataset("roneneldan/TinyStories", split="train", streaming=True)
    
    def collate_fn(batch):
        texts = [item['text'] for item in batch]
        encodings = tokenizer(
            texts, 
            truncation=True, 
            padding='max_length', 
            max_length=CONFIG['seq_len'] + 1,
            return_tensors='pt'
        )
        input_ids = encodings['input_ids']
        return input_ids[:, :-1], input_ids[:, 1:] # x, y

    # å–ä¸€ä¸ªç®€å•çš„è¿­ä»£å™¨
    dataloader = DataLoader(dataset, batch_size=CONFIG['batch_size'], collate_fn=collate_fn)
    return dataloader

# ==========================================
# 4. è®­ç»ƒä¸è¯„ä¼°å¾ªç¯
# ==========================================
def train_and_evaluate(model, name, dataloader):
    print(f"\nâš¡ Training Model: [{name}]")
    print("-" * 40)
    
    model = model.to(CONFIG['device']).bfloat16() # H100 å»ºè®®ä½¿ç”¨ BF16
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    
    print("  ğŸ”¥ Warming up (Compiling Triton Kernels)...")
    # å…ˆç©ºè·‘ 5 æ­¥ï¼Œè§¦å‘ Triton ç¼–è¯‘ï¼Œä¸è®¡å…¥æ—¶é—´
    warmup_steps = 5
    warmup_iter = iter(dataloader)
    for _ in range(warmup_steps):
        try:
            wx, wy = next(warmup_iter)
            wx, wy = wx.to(CONFIG['device']), wy.to(CONFIG['device'])
            optimizer.zero_grad()
            loss = criterion(model(wx).view(-1, CONFIG['vocab_size']), wy.view(-1))
            loss.backward()
            optimizer.step()
        except StopIteration:
            break
    
    print("  ğŸš€ Benchmark started!")
    # é‡ç½®è®¡æ—¶å™¨å’Œè®¡æ•°å™¨
    torch.cuda.synchronize() # ç¡®ä¿ GPU æ­¤æ—¶ç©ºé—²
    start_time = time.time()
    total_loss = 0
    
    # é¢„çƒ­æ˜¾å­˜
    # torch.cuda.empty_cache()
    # torch.cuda.reset_peak_memory_stats()
    # start_event = torch.cuda.Event(enable_timing=True)
    # end_event = torch.cuda.Event(enable_timing=True)
    
    total_loss = 0
    start_time = time.time()
    
    step = 0
    for x, y in dataloader:
        if step >= CONFIG['steps']: break
        
        x, y = x.to(CONFIG['device']), y.to(CONFIG['device'])
        
        optimizer.zero_grad()
        
        # è®°å½•å‰å‘ä¼ æ’­æ˜¾å­˜
        if step == 10: 
            torch.cuda.reset_peak_memory_stats()
            
        logits = model(x)
        loss = criterion(logits.view(-1, CONFIG['vocab_size']), y.view(-1))
        
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        if step % 20 == 0:
            print(f"  Step {step}/{CONFIG['steps']} | Loss: {loss.item():.4f}")
        step += 1

    end_time = time.time()
    avg_loss = total_loss / CONFIG['steps']
    ppl = math.exp(avg_loss)
    
    # ç»Ÿè®¡æ•°æ®
    total_tokens = CONFIG['batch_size'] * CONFIG['seq_len'] * CONFIG['steps']
    duration = end_time - start_time
    tokens_per_sec = total_tokens / duration
    max_mem = torch.cuda.max_memory_allocated() / 1024**2 # MB
    
    print(f"\nğŸ“Š [{name}] Results:")
    print(f"  > Perplexity (PPL): {ppl:.2f}")
    print(f"  > Speed: {tokens_per_sec:.0f} tokens/sec")
    print(f"  > Peak Memory: {max_mem:.0f} MB")
    
    return {
        "model": name,
        "ppl": ppl,
        "speed": tokens_per_sec,
        "mem": max_mem
    }

# ==========================================
# 5. ä¸»ç¨‹åº
# ==========================================
if __name__ == "__main__":
    dataloader = get_dataloader()
    
    results = []
    
    # 1. æµ‹è¯• RNN (åŸºå‡†)
    rnn = RNNBaseline()
    results.append(train_and_evaluate(rnn, "RNN (LSTM)", dataloader))
    del rnn
    
    # 2. æµ‹è¯• Transformer (æ ‡å‡†)
    # æ³¨æ„ï¼šå¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå¯èƒ½éœ€è¦å‡å° batch_size
    tf = TransformerBaseline()
    results.append(train_and_evaluate(tf, "Transformer (O(N^2))", dataloader))
    del tf
    
    # 3. æµ‹è¯• Hybrid/Linear (å‰æ²¿)
    gla = HybridGLA()
    results.append(train_and_evaluate(gla, "Hybrid (GLA/Linear)", dataloader))
    del gla
    
    print("\n\nğŸ† æœ€ç»ˆå¯¹æ¯”æ€»ç»“ (H100 Performance)")
    print("=" * 60)
    print(f"{'Model':<20} | {'PPL (Lower Better)':<20} | {'Speed (Higher Better)':<20} | {'Mem (MB)':<10}")
    print("-" * 60)
    for r in results:
        print(f"{r['model']:<20} | {r['ppl']:<20.2f} | {r['speed']:<20.0f} | {r['mem']:<10.0f}")
    print("=" * 60)