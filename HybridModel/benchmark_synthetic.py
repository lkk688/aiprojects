import torch
import torch.nn as nn
import time
import os
import shutil
from fla.layers import GatedLinearAttention

# ==========================================
# 0. å¼ºåˆ¶æ¸…ç† Triton ç¼“å­˜ (ä¿®å¤é€Ÿåº¦é—®é¢˜çš„å…³é”®)
# ==========================================
triton_cache = os.path.expanduser("~/.triton/cache")
if os.path.exists(triton_cache):
    print(f"ğŸ§¹ Clearing Triton cache at {triton_cache} to fix kernel issues...")
    try:
        shutil.rmtree(triton_cache)
    except:
        print("   (Warning: Could not clear cache, requires manual deletion)")

# ==========================================
# 1. é…ç½®ï¼šé’ˆå¯¹ H100 ä¼˜åŒ–çš„åˆæˆä»»åŠ¡
# ==========================================
CONFIG = {
    "vocab_size": 128,      # æå°è¯è¡¨ï¼Œä¿è¯èƒ½å­¦ä¼š
    "d_model": 256,
    "n_layers": 2,
    "n_heads": 4,
    "seq_len": 4096,        # é•¿åºåˆ—ï¼Œæµ‹è¯•è®°å¿†åŠ›
    "batch_size": 8,
    "steps": 200,           # è¶³å¤Ÿå­¦ä¼šç®€å•è§„å¾‹
    "device": "cuda"
}

print(f"ğŸš€ Running Synthetic Benchmark (Focus: Induction & Speed)")

# ==========================================
# 2. æ•°æ®ç”Ÿæˆå™¨ï¼šä¸“é—¨è®­ç»ƒ Induction èƒ½åŠ›
# ==========================================
def get_synthetic_batch():
    """
    ç”Ÿæˆ "Induction" æ•°æ®ï¼š
    [Key, Val, ..., Key] -> Target: Val
    è¿™è¿«ä½¿æ¨¡å‹å­¦ä¼š 'æŸ¥æ‰¾å†å²'
    """
    #å‰åŠæ®µéšæœº
    half_len = CONFIG['seq_len'] // 2
    rand_tokens = torch.randint(0, CONFIG['vocab_size']-1, (CONFIG['batch_size'], half_len), device=CONFIG['device'])
    
    # ååŠæ®µå®Œå…¨å¤åˆ¶å‰åŠæ®µ (Induction Task)
    # Input:  [A B C ... A B C]
    # Target: [B C ... A B C .]
    input_ids = torch.cat([rand_tokens, rand_tokens], dim=1)
    
    # æ„é€  Target (å³ç§»ä¸€ä½)
    target_ids = torch.roll(input_ids, shifts=-1, dims=1)
    target_ids[:, -1] = -100 # æœ€åä¸€ä½ä¸é¢„æµ‹
    
    return input_ids, target_ids

# ==========================================
# 3. æ¨¡å‹å®šä¹‰
# ==========================================
class HybridModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.embed = nn.Embedding(CONFIG['vocab_size'], CONFIG['d_model'])
        self.layers = nn.ModuleList([
            GLABlock(CONFIG['d_model'], CONFIG['n_heads']) 
            for _ in range(CONFIG['n_layers'])
        ])
        self.head = nn.Linear(CONFIG['d_model'], CONFIG['vocab_size'])

    def forward(self, x):
        x = self.embed(x)
        for layer in self.layers:
            x = layer(x)
        return self.head(x)

class GLABlock(nn.Module):
    def __init__(self, h, heads):
        super().__init__()
        self.norm = nn.LayerNorm(h)
        # æ˜¾å¼æŒ‡å®šå‚æ•°ï¼Œé˜²æ­¢æŠ¥é”™
        self.attn = GatedLinearAttention(
            hidden_size=h, 
            num_heads=heads, 
            mode='chunk' # H100 é«˜æ€§èƒ½æ¨¡å¼
        )
        self.mlp = nn.Sequential(nn.Linear(h, h*2), nn.GELU(), nn.Linear(h*2, h))

    def forward(self, x):
        # è¿™é‡Œçš„ [0] æ˜¯å– outputï¼Œä¸¢å¼ƒ state
        x = x + self.attn(self.norm(x))[0]
        x = x + self.mlp(self.norm(x))
        return x

# ==========================================
# 4. è®­ç»ƒä¸éªŒè¯å¾ªç¯
# ==========================================
def run():
    model = HybridModel().to(CONFIG['device']).bfloat16()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()

    print("\nâš¡ Training Hybrid Model on Induction Task...")
    model.train()
    
    # é¢„çƒ­ (è§¦å‘ç¼–è¯‘)
    print("  ğŸ”¥ Warming up JIT compiler...")
    x, y = get_synthetic_batch()
    for _ in range(5): model(x)
    torch.cuda.synchronize()
    
    start_time = time.time()
    total_tokens = 0
    
    for step in range(CONFIG['steps']):
        x, y = get_synthetic_batch()
        
        optimizer.zero_grad()
        logits = model(x)
        loss = criterion(logits.view(-1, CONFIG['vocab_size']), y.view(-1))
        loss.backward()
        optimizer.step()
        
        total_tokens += x.numel()
        
        if step % 20 == 0:
            print(f"  Step {step:03d} | Loss: {loss.item():.4f}")
            if loss.item() < 0.1:
                print("  âœ… Converged! (Learned Induction)")
                break
                
    duration = time.time() - start_time
    speed = total_tokens / duration
    
    print(f"\nğŸ“Š Results:")
    print(f"  > Final Loss: {loss.item():.4f} (Should be near 0)")
    print(f"  > Speed: {speed:.0f} tokens/sec")
    
    # ç®€å•çš„ Passkey æµ‹è¯•
    print("\nğŸ§ª Quick Passkey Test:")
    model.eval()
    # æ„é€ : "Key is 42 ... (4000 tokens) ... Key is" -> åº”è¯¥é¢„æµ‹ 42
    test_seq = torch.zeros((1, CONFIG['seq_len']), dtype=torch.long, device=CONFIG['device'])
    test_seq[0, 0] = 42  # Secret Key
    test_seq[0, -1] = 42 # Prompt
    
    with torch.no_grad():
        out = model(test_seq)
        pred = out[0, -2].argmax().item() # é¢„æµ‹å€’æ•°ç¬¬äºŒä¸ªä½ç½®çš„ä¸‹ä¸€ä¸ª
        
    print(f"  > Input: [42, 0, 0, ..., 42]")
    print(f"  > Target: 42")
    print(f"  > Pred:   {pred}")
    print(f"  > Result: {'âœ… Success' if pred == 42 else 'âŒ Fail'}")

if __name__ == "__main__":
    try:
        run()
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        print("ğŸ’¡ Hint: If speed is slow (~1000), Triton is broken. Try reinstalling fla.")