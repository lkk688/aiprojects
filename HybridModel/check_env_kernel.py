import torch
import time
from mamba_ssm.ops.selective_scan_interface import selective_scan_fn
from fla.ops.gated_linear_attention import chunk_gla

print(f"ğŸ” System Diagnosis:")
print(f"   Python: {torch.__version__}")
print(f"   CUDA:   {torch.version.cuda}")
print(f"   GPU:    {torch.cuda.get_device_name(0)}")

# ------------------------------------------------------
# 1. æµ‹è¯• Mamba å†…æ ¸ (Selective Scan)
# ------------------------------------------------------
print("\nğŸ§ª Testing Mamba Kernel (Selective Scan)...")
B, L, D, N = 16, 4096, 1024, 16
u = torch.randn(B, D, L, device='cuda', dtype=torch.bfloat16)
delta = torch.randn(B, D, L, device='cuda', dtype=torch.float32) # Mamba delta is fp32
A = torch.randn(D, N, device='cuda', dtype=torch.float32)
B_ = torch.randn(B, N, L, device='cuda', dtype=torch.bfloat16)
C = torch.randn(B, N, L, device='cuda', dtype=torch.bfloat16)
D_ = torch.randn(D, device='cuda', dtype=torch.float32)

torch.cuda.synchronize()
start = time.time()
# å¼ºåˆ¶è°ƒç”¨ CUDA å†…æ ¸
out = selective_scan_fn(u, delta, A, B_, C, D_, z=None, delta_bias=None, delta_softplus=True)
torch.cuda.synchronize()
dur = time.time() - start

print(f"   âœ… Mamba Kernel Status: RUNNING")
print(f"   â±ï¸ Execution Time: {dur*1000:.2f} ms")
if dur > 0.5: # æ­£å¸¸åº”è¯¥åœ¨ 10ms ä»¥å†…
    print("   âš ï¸ WARNING: Mamba is surprisingly slow. Is it falling back to CPU?")
else:
    print("   ğŸš€ SPEED: Excellent! (Hardware Accelerated)")

# ------------------------------------------------------
# 2. æµ‹è¯• FLA å†…æ ¸ (Chunk GLA)
# ------------------------------------------------------
print("\nğŸ§ª Testing FLA Kernel (Triton Chunk GLA)...")
q = torch.randn(B, L, 8, 128, device='cuda', dtype=torch.bfloat16) # [B, L, H, D]
k = torch.randn(B, L, 8, 128, device='cuda', dtype=torch.bfloat16)
v = torch.randn(B, L, 8, 128, device='cuda', dtype=torch.bfloat16)
g = torch.randn(B, L, 8, 128, device='cuda', dtype=torch.bfloat16)

# é¢„çƒ­ Triton ç¼–è¯‘å™¨
try:
    chunk_gla(q, k, v, g)
    torch.cuda.synchronize()
    
    start = time.time()
    chunk_gla(q, k, v, g)
    torch.cuda.synchronize()
    dur = time.time() - start
    
    print(f"   âœ… FLA Triton Kernel: COMPILED & RUNNING")
    print(f"   â±ï¸ Execution Time: {dur*1000:.2f} ms")
    if dur > 0.5:
         print("   âš ï¸ WARNING: FLA is slow.")
    else:
         print("   ğŸš€ SPEED: Excellent! (Triton Accelerated)")

except Exception as e:
    print(f"   âŒ FLA Kernel Failed: {e}")
    print("   ğŸ‘‰ Solution: Re-install fla with 'pip install .'")